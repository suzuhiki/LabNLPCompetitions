{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a5b3babc-323e-4655-a26b-995c4b569d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../../data/\"\n",
    "tmppath = \"../../tmp/02/\"\n",
    "outpath = \"./../output/\"\n",
    "settingpath = \"./../setting/\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bcba6018-f2c9-4794-a530-52a4dca37939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n",
      "0.14.1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(torch.__version__)  # 1.3.1\n",
    "print(torchtext.__version__)  # 0.5.0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9068daf-3d9a-429e-a7a1-664c5519f351",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d19ee-b911-416a-910f-6d71ec3a512e",
   "metadata": {},
   "source": [
    "## データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "865bd646-b9c0-4b54-9a54-562459201b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_text_file(path):\n",
    "    with open(path, mode=\"r\") as f:\n",
    "        result = f.read().splitlines()\n",
    "    return result\n",
    "\n",
    "def read_label_file(path):\n",
    "    result = np.loadtxt(path, dtype='int64')\n",
    "    return result\n",
    "\n",
    "def create_df(textpath, lablpath):\n",
    "    result = pd.DataFrame({'text': read_text_file(textpath),\n",
    "                           'label': read_label_file(lablpath)})\n",
    "    return result\n",
    "\n",
    "def create_testdf(textpath):\n",
    "    result = pd.DataFrame({'text': read_text_file(textpath)})\n",
    "    result[\"label\"] = [0]*len(result)\n",
    "    return result\n",
    "    \n",
    "train_df_origin = create_df(datapath + \"text.train.txt\", datapath + \"label.train.txt\")\n",
    "dev_df_origin = create_df(datapath + \"text.dev.txt\", datapath + \"label.dev.txt\")\n",
    "test_df_origin = create_testdf(datapath + \"text.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d8ab30da-1d6c-4707-863d-b0f30210c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0                     ぼけっとしてたらこんな時間。チャリあるから食べにでたいのに…      0\n",
      "1  今日の月も白くて明るい。昨日より雲が少なくてキレイな〜 と立ち止まる帰り道。チャリなし生活も...      1\n",
      "2                 早寝するつもりが飲み物がなくなりコンビニへ。ん、今日、風が涼しいな。      0\n",
      "3                                           眠い、眠れない。      0\n",
      "4    ただいま〜 って新体操してるやん!外食する気満々で家に何もないのに!テレビから離れられない…!      0\n"
     ]
    }
   ],
   "source": [
    "print(train_df_origin.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a9bd9600-6f31-405b-a437-1e9649c8fc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          text  label\n",
      "0        ハッピーセット、またプラレールが始まるからしばらくマックばかりになりそう。      0\n",
      "1               今日は天気が良いので外出がきもちよいです。秋晴れ良いですね。      0\n",
      "2  あぁ〜そうなんだ。。。 やっぱライブでは芸人みんなわちゃわちゃしてるとこが見たかったな      0\n",
      "3                               踊り場110話まできたぞこら      0\n",
      "4                         カウコン行かれる方、楽しんで下さい〜！！      0\n"
     ]
    }
   ],
   "source": [
    "print(test_df_origin.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f58a2-21e9-44b0-8b98-89f9aa44eb46",
   "metadata": {},
   "source": [
    "## テキストクリーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "095f2eb6-e47c-4a53-aa20-3f68c4d36008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['新体操', '表情筋', 'シャレード', 'モテキ', '小宮山', '夏樹', 'バッティングセンター', 'COWCOW', 'BIGBANG', '笑む', '立石', '生半可', '達者', 'たんと', 'しい', '外反母趾', '昔夢', 'チョコケーキ', '天地明察', '不確か', '連れ去る', '微か', '絡み付く', '投げ掛ける', '茶碗蒸し', 'シナモンロール', '́з', '2700', 'wy', '愛と誠', '在り来り', 'ミランダ', 'カー', 'お子様', '岩', '整骨院', 'テーピング', '丸腰', '整体', '釣れる', '自己否定', '自己肯定', '鬼太郎', '彦', '門', '豚骨', '茄子', '田楽', 'パトラッシュ', 'エビマヨ', '煮', 'ふじ', 'ピール', 'なか卯', 'ジャーナリスト', '祈り', 'さめる', 'メルヘン', '齧り付く', '同志', '求', '本が好き', '立ち読み', '流し読み', 'standardbookstore', 'ノーベル賞', '山中', '育む', '西田辺', '割り箸', 'ニーハイ', '歯痒い', '信託', '別口', '著作', '田中里奈', 'たなか', 'りな', 'ティーナカリーナ', 'キョンキョン', '悪の教典', 'ニモ', 'ケズ', '出家', 'がぶり', 'カブレ', 'たらふく', 'ホームパーティー', '祭りのあと', 'BARBEE BOYS', '差し込む', 'はだける', '生田斗真', 'てれび戦士', '精神年齢', '体年齢', '串カツ', '有耶無耶', '怪しむ', '挙式']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary(dict=\"full\").create()\n",
    "mode = tokenizer.Tokenizer.SplitMode.C\n",
    "\n",
    "clear_part_of_speech_list = [[\"助詞\", \"助動詞\"],[\"数詞\"]]\n",
    "\n",
    "with open(tmppath + \"stopwords.txt\") as f:\n",
    "    stopword_list = f.read().splitlines()\n",
    "\n",
    "# 出現頻度が少ない単語をstopwordとする\n",
    "def stopwords_occur(textlist, threshold):\n",
    "    morphemelist = [tokenizer_obj.tokenize(text, mode) for text in textlist]\n",
    "    words = []\n",
    "    for morpheme in morphemelist:\n",
    "        for word in morpheme:\n",
    "            words.append(word.normalized_form())\n",
    "    dic = collections.Counter(words)\n",
    "    dic = {key:value for key, value in dic.items() if value<= threshold}\n",
    "    return list(dic.keys())\n",
    "\n",
    "stopwords_occur = stopwords_occur(train_df_origin[\"text\"], 2)\n",
    "\n",
    "stopword_list = []\n",
    "\n",
    "stopword_list.extend(stopwords_occur)\n",
    "\n",
    "print(stopword_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2e35d1d7-2690-4dca-994f-148d7adcf60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text, mode, clear_part_of_speech_list, stopword_list):\n",
    "    words = []\n",
    "    for word in tokenizer_obj.tokenize(text, mode):\n",
    "        if word.part_of_speech()[0] not in clear_part_of_speech_list[0] and word.part_of_speech()[1] not in clear_part_of_speech_list[1] and word.normalized_form() not in stopword_list:\n",
    "            words.append(word.normalized_form())\n",
    "    return \" \".join(words)\n",
    "\n",
    "def df_cleaning(df):\n",
    "    result_df = df.copy()\n",
    "    result_df['text'] = df['text'].map(lambda x: text_cleaning(x, mode, clear_part_of_speech_list, stopword_list))\n",
    "    return result_df\n",
    "\n",
    "train_df = df_cleaning(train_df_origin)\n",
    "dev_df = df_cleaning(dev_df_origin)\n",
    "test_df = df_cleaning(test_df_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f065ed8d-8aee-4a87-8018-bda31f043cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0               ぼけっと 為る こんな 時間 。 ちゃり 有る 食べる 出る . . .      0\n",
      "1  今日 月 白い 明るい 。 昨日 雲 少ない 奇麗   立ち止まる 帰り道 。 ちゃり なし...      1\n",
      "2        早寝 為る 積もり 飲み物 なくなる コンビニ 。 んっ 、 今日 、 風 涼しい 。      0\n",
      "3                                          眠い 、 眠る 。      0\n",
      "4    只今 〜   為る ! 外食 為る 気 満々 家 何 無い ! テレビ 離れる . . . !      0\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "920c8362-691c-4215-a4ca-2022375d49bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  text  label\n",
      "0               、 又 始まる 暫く マック 成る そう 。      0\n",
      "1        今日 天気 良い 外出 気持ち 良い 。 秋晴れ 良い 。      0\n",
      "2  あー そう 。 。 。   矢張り ライブ 芸人 皆 為る とこ 見る      0\n",
      "3                          踊り場 話 来る 此れ      0\n",
      "4            カウコン 行く 方 、 楽しむ 下さる 〜 ! !      0\n"
     ]
    }
   ],
   "source": [
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8434af-82e6-4bc7-a972-33c7b7871acf",
   "metadata": {},
   "source": [
    "## 辞書作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "86b1833d-c415-4d9a-b7d2-b7b16f201d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'ぼけっと', '為る', 'こんな', '時間', '。', 'ちゃり', '有る', '食べる', '出る', '.', '今日', '月', '白い', '明るい', '昨日', '雲', '少ない', '奇麗', '立ち止まる', '帰り道', 'なし', '生活', '悪い', '無い', '早寝', '積もり', '飲み物', 'なくなる']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "counter = Counter()\n",
    "for text in train_df[\"text\"]:\n",
    "    counter.update(text.split())\n",
    "\n",
    "voc = vocab(counter, specials=(['<unk>', '<pad>']))\n",
    "voc.set_default_index(voc['<unk>'])\n",
    "\n",
    "print(voc.get_itos()[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab08bfc-ec1c-4aa7-950e-f9738f736fc1",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "43266482-05b2-4543-a5c1-d064f4bbd84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.transforms as T\n",
    "\n",
    "text_transform = T.Sequential(\n",
    "    T.VocabTransform(voc),\n",
    "    T.ToTensor(padding_value=voc['<pad>'])\n",
    ")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts = text_transform([text.split() for (text, label) in batch])\n",
    "    # texts = torch.t(texts)\n",
    "    labels = torch.tensor([label+2 for (text, label) in batch], dtype=torch.long)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "76be0115-ff0a-46ed-9488-1d2915b1cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(train_df.values, batch_size=batch_size, shuffle=True, collate_fn=collate_batch, num_workers=4)\n",
    "dev_loader = DataLoader(dev_df.values, batch_size=batch_size, shuffle=False, collate_fn=collate_batch, num_workers=4)\n",
    "test_loader = DataLoader(test_df.values, batch_size=batch_size, shuffle=False, collate_fn=collate_batch, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e5e53289-37c0-4bf3-a827-8026478603f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512, 54])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "x, t = batch\n",
    "print(t.shape)\n",
    "print(x.shape)\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "x, t = batch\n",
    "print(t.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c588b-4312-42ee-ab74-6a0f8e40257b",
   "metadata": {},
   "source": [
    "# モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "36339660-fefc-45ca-9374-9fab61aedc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200 # 単語の埋め込み次元数\n",
    "LSTM_DIM = 128 # LSTMの隠れ層の次元数\n",
    "VOCAB_SIZE =len(voc) # 全単語数\n",
    "TAG_SIZE = 5 # ネットワークの最後のサイズ\n",
    "DA = 64 # AttentionをNeural Networkで計算する際の重み行列のサイズ\n",
    "R = 3 # Attentionを３層重ねて見る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "36e280bd-911c-42c9-aa6c-f41715895c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, lstm_dim, vocab_size):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # 単語ベクトルを誤差逆伝播で更新させないためにrequires_gradをFalseに設定する\n",
    "        # self.word_embeddings.requires_grad_ = False\n",
    "\n",
    "        # bidirectional=Trueで双方向のLSTMを利用\n",
    "        self.bilstm = nn.LSTM(embedding_dim, lstm_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embeds = self.word_embeddings(text)\n",
    "\n",
    "        # 各隠れ層のベクトルがほしいので第１戻り値を受け取る\n",
    "        out, _ = self.bilstm(embeds)\n",
    "\n",
    "        # 前方向と後ろ方向の各隠れ層のベクトルを結合したままの状態で返す\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "14d88424-fb2e-462d-ac9d-2d820a385379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, lstm_dim, da, r):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.da = da\n",
    "        self.r = r\n",
    "        self.main = nn.Sequential(\n",
    "            # Bidirectionalなので各隠れ層のベクトルの次元は２倍のサイズ\n",
    "            nn.Linear(lstm_dim * 2, da), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(da, r)\n",
    "        )\n",
    "    def forward(self, out):\n",
    "        return F.softmax(self.main(out), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "9a69e533-36c8-484e-b11b-097e78f1a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionClassifier(nn.Module):\n",
    "    def __init__(self, lstm_dim, da, r, tagset_size):\n",
    "        super(SelfAttentionClassifier, self).__init__()\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.r = r\n",
    "        self.attn = SelfAttention(lstm_dim, da, r)\n",
    "        self.main = nn.Linear(lstm_dim * 6, tagset_size)\n",
    "\n",
    "    def forward(self, out):\n",
    "        attention_weight = self.attn(out)\n",
    "        m1 = (out * attention_weight[:,:,0].unsqueeze(2)).sum(dim=1)\n",
    "        m2 = (out * attention_weight[:,:,1].unsqueeze(2)).sum(dim=1)\n",
    "        m3 = (out * attention_weight[:,:,2].unsqueeze(2)).sum(dim=1)\n",
    "        feats = torch.cat([m1, m2, m3], dim=1)\n",
    "        return F.log_softmax(self.main(feats)), attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5c5e5-3c24-4adf-a9c6-4242ed0b1e2a",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "3366dc61-def7-477b-bcb3-7465e0bc58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from itertools import chain\n",
    "\n",
    "encoder = BiLSTMEncoder(EMBEDDING_DIM, LSTM_DIM, VOCAB_SIZE).to(device)\n",
    "classifier = SelfAttentionClassifier(LSTM_DIM, DA, R, TAG_SIZE).to(device)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# 複数のモデルを from itertools import chain で囲えばoptimizerをまとめて1つにできる\n",
    "optimizer = optim.Adam(chain(encoder.parameters(), classifier.parameters()), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "57d78996-244c-4ab8-93b2-8c9bd8e5437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \t loss 1.5526013374328613 \t val loss 1.6318556070327759\n",
      "epoch 1 \t loss 1.5020326375961304 \t val loss 1.552908182144165\n",
      "epoch 2 \t loss 1.5608993768692017 \t val loss 1.570878028869629\n",
      "epoch 3 \t loss 1.5135682821273804 \t val loss 1.5792490243911743\n",
      "epoch 4 \t loss 1.4873039722442627 \t val loss 1.5867362022399902\n",
      "epoch 5 \t loss 1.511178970336914 \t val loss 1.6014349460601807\n",
      "epoch 6 \t loss 1.5230742692947388 \t val loss 1.5836420059204102\n",
      "epoch 7 \t loss 1.4571787118911743 \t val loss 1.5693082809448242\n",
      "epoch 8 \t loss 1.4610240459442139 \t val loss 1.599522590637207\n",
      "epoch 9 \t loss 1.4555978775024414 \t val loss 1.5806852579116821\n",
      "epoch 10 \t loss 1.387213110923767 \t val loss 1.6015608310699463\n",
      "epoch 11 \t loss 1.356120228767395 \t val loss 1.6132428646087646\n",
      "epoch 12 \t loss 1.3693211078643799 \t val loss 1.6226720809936523\n",
      "epoch 13 \t loss 1.4051826000213623 \t val loss 1.6628316640853882\n",
      "epoch 14 \t loss 1.3321012258529663 \t val loss 1.6423391103744507\n",
      "epoch 15 \t loss 1.3431711196899414 \t val loss 1.6349657773971558\n",
      "epoch 16 \t loss 1.2558680772781372 \t val loss 1.6559877395629883\n",
      "epoch 17 \t loss 1.2673543691635132 \t val loss 1.6378319263458252\n",
      "epoch 18 \t loss 1.3329997062683105 \t val loss 1.62191903591156\n",
      "epoch 19 \t loss 1.274638295173645 \t val loss 1.7408573627471924\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    all_loss = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        batch_loss = 0\n",
    "        encoder.zero_grad()\n",
    "        classifier.zero_grad()\n",
    "\n",
    "        text_tensor, label_tensor = batch\n",
    "        out = encoder(text_tensor.to(device))\n",
    "        score, attn = classifier(out)\n",
    "        batch_loss = loss_function(score, label_tensor.to(device))\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        all_loss = batch_loss.item()\n",
    "        \n",
    "        # valloss\n",
    "        if idx == 0:\n",
    "            with torch.no_grad():\n",
    "                for batch in dev_loader:\n",
    "                    text_tensor, label_tensor = batch\n",
    "                    out = encoder(text_tensor.to(device))\n",
    "                    score, _ = classifier(out)\n",
    "                    val_loss = loss_function(score, label_tensor.to(device)).item()\n",
    "            \n",
    "    print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss, \"\\t\", \"val loss\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ed3bd-94ce-4ee5-ae9d-9a266ee868f5",
   "metadata": {},
   "source": [
    "# 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "572e268b-d9dc-486d-abb9-3d888060d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.22      0.16       169\n",
      "           1       0.36      0.23      0.28       649\n",
      "           2       0.53      0.30      0.38      1161\n",
      "           3       0.27      0.51      0.36       447\n",
      "           4       0.10      0.39      0.16        74\n",
      "\n",
      "    accuracy                           0.32      2500\n",
      "   macro avg       0.28      0.33      0.27      2500\n",
      "weighted avg       0.40      0.32      0.33      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "answer = []\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "        text_tensor, label_tensor = batch\n",
    "\n",
    "        out = encoder(text_tensor.to(device))\n",
    "        score, _ = classifier(out)\n",
    "        _, pred = torch.max(score, 1)\n",
    "\n",
    "        prediction += list(pred.cpu().numpy())\n",
    "        answer += list(label_tensor.cpu().numpy())\n",
    "print(classification_report(prediction, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a790cd5-f019-486c-a735-169a2876ccc1",
   "metadata": {},
   "source": [
    "# 提出用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e9ad89e0-4d9d-40dc-8d14-a1d787b145e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  text  label\n",
      "0               、 又 始まる 暫く マック 成る そう 。      0\n",
      "1        今日 天気 良い 外出 気持ち 良い 。 秋晴れ 良い 。      1\n",
      "2  あー そう 。 。 。   矢張り ライブ 芸人 皆 為る とこ 見る      0\n",
      "3                          踊り場 話 来る 此れ     -1\n",
      "4            カウコン 行く 方 、 楽しむ 下さる 〜 ! !      1\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "test_result_df = test_df.copy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        text_tensor, _ = batch\n",
    "\n",
    "        out = encoder(text_tensor.to(device))\n",
    "        score, _ = classifier(out)\n",
    "        _, pred = torch.max(score, 1)\n",
    "\n",
    "        prediction += list(pred.cpu().numpy())\n",
    "\n",
    "test_result_df[\"label\"] = [pred-2 for pred in prediction]\n",
    "print(test_result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "975dc5b0-7874-4e95-a244-a96e53dc8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outpath + \"test.txt\",\"w\") as f:\n",
    "    f.writelines(\"\\n\".join([str(pred-2) for pred in prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3bc62-12be-419f-aa8c-6c4ad55406cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
