{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5b3babc-323e-4655-a26b-995c4b569d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../data/\"\n",
    "tmppath = \"../tmp/02/\"\n",
    "outpath = \"./output/\"\n",
    "settingpath = \"./setting/\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bcba6018-f2c9-4794-a530-52a4dca37939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n",
      "0.14.1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(torch.__version__)  # 1.3.1\n",
    "print(torchtext.__version__)  # 0.5.0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9068daf-3d9a-429e-a7a1-664c5519f351",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d19ee-b911-416a-910f-6d71ec3a512e",
   "metadata": {},
   "source": [
    "## データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "865bd646-b9c0-4b54-9a54-562459201b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_text_file(path):\n",
    "    with open(path, mode=\"r\") as f:\n",
    "        result = f.read().splitlines()\n",
    "    return result\n",
    "\n",
    "def read_label_file(path):\n",
    "    result = np.loadtxt(path, dtype='int64')\n",
    "    return result\n",
    "\n",
    "def create_df(textpath, lablpath):\n",
    "    result = pd.DataFrame({'text': read_text_file(textpath),\n",
    "                           'label': read_label_file(lablpath)})\n",
    "    return result\n",
    "    \n",
    "train_df_origin = create_df(datapath + \"text.train.txt\", datapath + \"label.train.txt\")\n",
    "dev_df_origin = create_df(datapath + \"text.dev.txt\", datapath + \"label.dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8ab30da-1d6c-4707-863d-b0f30210c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0                     ぼけっとしてたらこんな時間。チャリあるから食べにでたいのに…      0\n",
      "1  今日の月も白くて明るい。昨日より雲が少なくてキレイな〜 と立ち止まる帰り道。チャリなし生活も...      1\n",
      "2                 早寝するつもりが飲み物がなくなりコンビニへ。ん、今日、風が涼しいな。      0\n",
      "3                                           眠い、眠れない。      0\n",
      "4    ただいま〜 って新体操してるやん!外食する気満々で家に何もないのに!テレビから離れられない…!      0\n"
     ]
    }
   ],
   "source": [
    "print(train_df_origin.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f58a2-21e9-44b0-8b98-89f9aa44eb46",
   "metadata": {},
   "source": [
    "## テキストクリーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "095f2eb6-e47c-4a53-aa20-3f68c4d36008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['。', '、', '.', '為る', '成る', '居る', 'とこ', ':', '/', '_', '-', '〜', '(', ')', '私', '御', '」', '「', '人', '物', 'ー', '言う', 'こと', '見る', '行く', '・', 'さん', 'ちゃん', 'そう', 'よう', ';', '`', '分', '今', '今日', '日', '有る', '又', '来る', '思う', '此の', '時', 'あそこ', 'あたり', 'あちら', 'あっち', 'あと', 'あな', 'あなた', 'あれ', 'いくつ', 'いつ', 'いま', 'いや', 'いろいろ', 'うち', 'おおまか', 'おまえ', 'おれ', 'がい', 'かく', 'かたち', 'かやの', 'から', 'がら', 'きた', 'くせ', 'ここ', 'こっち', 'こと', 'ごと', 'こちら', 'ごっちゃ', 'これ', 'これら', 'ごろ', 'さまざま', 'さらい', 'さん', 'しかた', 'しよう', 'すか', 'ずつ', 'すね', 'すべて', 'ぜんぶ', 'そう', 'そこ', 'そちら', 'そっち', 'そで', 'それ', 'それぞれ', 'それなり', 'たくさん', 'たち', 'たび', 'ため', 'だめ', 'ちゃ']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary(dict=\"full\").create()\n",
    "mode = tokenizer.Tokenizer.SplitMode.C\n",
    "\n",
    "clear_part_of_speech_list = [[\"助詞\", \"助動詞\"],[\"数詞\"]]\n",
    "\n",
    "with open(tmppath + \"stopwords.txt\") as f:\n",
    "    stopword_list = f.read().splitlines()\n",
    "\n",
    "# 出現頻度が少ない単語をstopwordとする\n",
    "def stopwords_occur(textlist, threshold):\n",
    "    morphemelist = [tokenizer_obj.tokenize(text, mode) for text in textlist]\n",
    "    words = []\n",
    "    for morpheme in morphemelist:\n",
    "        for word in morpheme:\n",
    "            words.append(word.normalized_form())\n",
    "    dic = collections.Counter(words)\n",
    "    dic = {key:value for key, value in dic.items() if value<= threshold}\n",
    "    return list(dic.keys())\n",
    "\n",
    "stopwords_occur = stopwords_occur(train_df_origin[\"text\"], 2)\n",
    "\n",
    "stopword_list.extend(stopwords_occur)\n",
    "\n",
    "print(stopword_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e35d1d7-2690-4dca-994f-148d7adcf60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text, mode, clear_part_of_speech_list, stopword_list):\n",
    "    words = []\n",
    "    for word in tokenizer_obj.tokenize(text, mode):\n",
    "        if word.part_of_speech()[0] not in clear_part_of_speech_list[0] and word.part_of_speech()[1] not in clear_part_of_speech_list[1] and word.normalized_form() not in stopword_list:\n",
    "            words.append(word.normalized_form())\n",
    "    return \" \".join(words)\n",
    "\n",
    "def df_cleaning(df):\n",
    "    result_df = df.copy()\n",
    "    result_df['text'] = df['text'].map(lambda x: text_cleaning(x, mode, clear_part_of_speech_list, stopword_list))\n",
    "    return result_df\n",
    "\n",
    "train_df = df_cleaning(train_df_origin)\n",
    "dev_df = df_cleaning(dev_df_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f065ed8d-8aee-4a87-8018-bda31f043cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             text  label\n",
      "0                             ぼけっと こんな ちゃり 食べる 出る      0\n",
      "1  白い 明るい 昨日 雲 少ない 奇麗   立ち止まる 帰り道 ちゃり なし 生活 悪い 無い      1\n",
      "2                   早寝 積もり 飲み物 なくなる コンビニ んっ 風 涼しい      0\n",
      "3                                           眠い 眠る      0\n",
      "4                     只今   ! 外食 満々 無い ! テレビ 離れる !      0\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8434af-82e6-4bc7-a972-33c7b7871acf",
   "metadata": {},
   "source": [
    "## 辞書作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "86b1833d-c415-4d9a-b7d2-b7b16f201d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'ぼけっと', 'こんな', 'ちゃり', '食べる', '出る', '白い', '明るい', '昨日', '雲', '少ない', '奇麗', '立ち止まる', '帰り道', 'なし', '生活', '悪い', '無い', '早寝', '積もり', '飲み物', 'なくなる', 'コンビニ', 'んっ', '風', '涼しい', '眠い', '眠る', '只今']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "counter = Counter()\n",
    "for text in train_df[\"text\"]:\n",
    "    counter.update(text.split())\n",
    "\n",
    "voc = vocab(counter, specials=(['<unk>', '<pad>']))\n",
    "voc.set_default_index(voc['<unk>'])\n",
    "\n",
    "print(voc.get_itos()[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab08bfc-ec1c-4aa7-950e-f9738f736fc1",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "43266482-05b2-4543-a5c1-d064f4bbd84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.transforms as T\n",
    "\n",
    "text_transform = T.Sequential(\n",
    "    T.VocabTransform(voc),\n",
    "    T.ToTensor(padding_value=voc['<pad>'])\n",
    ")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts = text_transform([text.split() for (text, label) in batch])\n",
    "    texts = torch.t(texts)\n",
    "    labels = torch.tensor([label+2 for (text, label) in batch], dtype=torch.long)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "76be0115-ff0a-46ed-9488-1d2915b1cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_df.values, batch_size=batch_size, shuffle=True, collate_fn=collate_batch, num_workers=4)\n",
    "dev_loader = DataLoader(dev_df.values, batch_size=batch_size, shuffle=False, collate_fn=collate_batch, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5e53289-37c0-4bf3-a827-8026478603f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "x, t = batch\n",
    "print(t.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c588b-4312-42ee-ab74-6a0f8e40257b",
   "metadata": {},
   "source": [
    "# モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "36e280bd-911c-42c9-aa6c-f41715895c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SAN(pl.LightningModule):\n",
    "\n",
    "    # 埋め込み層, 隠れ層, 全結合層の定義 \n",
    "    def __init__(self, n_tokens, n_embed, n_heads, n_layers, n_output, dropout):\n",
    "        super(SAN, self).__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.embed = nn.Embedding(num_embeddings=n_tokens, embedding_dim=n_embed, padding_idx=voc['<pad>'])\n",
    "        self.pos_encoder = PositionalEncoding(num_embeddings=n_tokens, embedding_dim=n_embed, dropout=dropout)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=n_embed, nhead=n_heads, dim_feedforward=n_embed*4, dropout=dropout)\n",
    "        self.san = nn.TransformerEncoder(encoder_layer=enc_layer, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(in_features=n_embed, out_features=n_output)\n",
    "    \n",
    "    # 順伝播\n",
    "    def forward(self, x):\n",
    "        e = self.pos_encoder(self.embed(x) * math.sqrt(self.n_embed))\n",
    "        o = self.san(e)\n",
    "        return self.fc(o.mean(dim=0))\n",
    "\n",
    "    # 訓練用データのバッチを受け取って損失を計算\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        loss = self.lossfun(y, t)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    # 検証用データのバッチを受け取って損失を計算\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        loss = self.lossfun(y, t)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    # 評価用データのバッチを受け取って分類の正解率を計算\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        y = torch.argmax(y, dim=1)\n",
    "        accuracy = torch.sum(t == y).item() / (len(y) * 1.0)\n",
    "        self.log(\"test_acc\", accuracy)\n",
    "\n",
    "    # 損失関数を設定\n",
    "    def lossfun(self, y, t):\n",
    "        return F.cross_entropy(y, t)\n",
    "\n",
    "    # 最適化手法を設定\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim, dropout):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        pe = torch.zeros(num_embeddings, 1, embedding_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5c5e5-3c24-4adf-a9c6-4242ed0b1e2a",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "36339660-fefc-45ca-9374-9fab61aedc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10670\n",
      "SAN(\n",
      "  (embed): Embedding(10670, 256, padding_idx=1)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (san): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_tokens = len(voc)\n",
    "print(n_tokens)\n",
    "n_embed = 256\n",
    "n_heads = 4\n",
    "n_layers = 4\n",
    "n_output = 5\n",
    "dropout = 0.2\n",
    "\n",
    "model = SAN(n_tokens, n_embed, n_heads, n_layers, n_output, dropout)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "57d78996-244c-4ab8-93b2-8c9bd8e5437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | embed       | Embedding          | 2.7 M \n",
      "1 | pos_encoder | PositionalEncoding | 0     \n",
      "2 | san         | TransformerEncoder | 3.2 M \n",
      "3 | fc          | Linear             | 1.3 K \n",
      "---------------------------------------------------\n",
      "5.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.9 M     Total params\n",
      "23.567    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab05ac4220c6496e846c22a791dfbec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ベストモデル:  /workspace/DockerML_sandbox/lab_competition/02/model/epoch=4-step=1175.ckpt\n",
      "ベストモデルの検証用データにおける損失:  tensor(1.5349, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 訓練中にモデルを保存するための設定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    # 検証用データにおける損失が最も小さいモデルを保存する\n",
    "    monitor=\"val_loss\", mode=\"min\", save_top_k=1,\n",
    "    # モデルファイル（重みのみ）を \"model\" というディレクトリに保存する\n",
    "    save_weights_only=True, dirpath=\"model/\"\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# 訓練\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=40, callbacks=[checkpoint])\n",
    "trainer.fit(model, train_loader, dev_loader)\n",
    "\n",
    "# ベストモデルの確認\n",
    "print(\"ベストモデル: \", checkpoint.best_model_path)\n",
    "print(\"ベストモデルの検証用データにおける損失: \", checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04707d96-0473-4838-89d8-b3670c48ba29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e268b-d9dc-486d-abb9-3d888060d24b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
